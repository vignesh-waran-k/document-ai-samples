{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05597417-c7d3-4132-99d9-76591e72b58b",
   "metadata": {},
   "source": [
    "# Date Entity Normalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad09b57c-8166-4662-b33d-dd883bcd4b2e",
   "metadata": {},
   "source": [
    "* Author: docai-incubator@google.com\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45325f56-279d-480d-af14-f138181d3efa",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "\n",
    "This tool is not supported by the Google engineering team or product team. It is provided and supported on a best-effort basis by the DocAI Incubator Team. No guarantees of performance are implied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cc7ea6-79b7-4905-a77f-6f5dec1de471",
   "metadata": {},
   "source": [
    "## Purpose and Description\n",
    "\n",
    "This tool updates the values of normalized dates in entities within the Document AI JSON output. It aids in identifying the actual date format, such as MM/DD/YYYY or DD/MM/YYYY, through a heuristic approach. Upon successful identification, the tool updates all date values in the JSON to maintain a consistent format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70169fe9-1709-4050-8f1c-3050cef19566",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "1. Vertex AI Notebook or Google Colab\n",
    "2. GCS bucket for processing of  the input json and output json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174b2cf3-04ad-45e4-b0b0-b279990ea211",
   "metadata": {},
   "source": [
    "## Step by Step procedure "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8983f13d-91d8-48cc-8f79-87c1cf309cc3",
   "metadata": {},
   "source": [
    "### 1. Install the required libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f1fa99b-07ba-427c-8c03-520efbb81161",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: google-cloud-storage in /opt/conda/lib/python3.7/site-packages (2.11.0)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage) (2.28.2)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage) (2.17.1)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage) (2.3.3)\n",
      "Requirement already satisfied: google-resumable-media>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage) (2.6.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage) (2.31.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage) (1.60.0)\n",
      "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage)\n",
      "  Using cached protobuf-4.24.4-cp37-abi3-manylinux2014_x86_64.whl.metadata (540 bytes)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (4.9)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media>=2.6.0->google-cloud-storage) (1.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2024.2.2)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage) (0.5.0)\n",
      "Using cached protobuf-4.24.4-cp37-abi3-manylinux2014_x86_64.whl (311 kB)\n",
      "\u001b[33mWARNING: Error parsing requirements for fastjsonschema: [Errno 2] No such file or directory: '/opt/conda/lib/python3.7/site-packages/fastjsonschema-2.18.0.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.0\n",
      "    Uninstalling protobuf-3.20.0:\n",
      "      Successfully uninstalled protobuf-3.20.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "apache-beam 2.46.0 requires zstandard<1,>=0.18.0, which is not installed.\n",
      "apache-beam 2.46.0 requires protobuf<4,>3.12.2, but you have protobuf 4.24.4 which is incompatible.\n",
      "google-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.2 which is incompatible.\n",
      "google-cloud-bigquery 2.34.4 requires protobuf<4.0.0dev,>=3.12.0, but you have protobuf 4.24.4 which is incompatible.\n",
      "google-cloud-bigtable 1.7.3 requires protobuf<4.0.0dev, but you have protobuf 4.24.4 which is incompatible.\n",
      "google-cloud-datastore 1.15.5 requires protobuf<4.0.0dev, but you have protobuf 4.24.4 which is incompatible.\n",
      "google-cloud-documentai-toolbox 0.12.0a0 requires google-cloud-bigquery<4.0.0dev,>=3.5.0, but you have google-cloud-bigquery 2.34.4 which is incompatible.\n",
      "google-cloud-language 1.3.2 requires protobuf<4.0.0dev, but you have protobuf 4.24.4 which is incompatible.\n",
      "google-cloud-videointelligence 1.16.3 requires protobuf<4.0.0dev, but you have protobuf 4.24.4 which is incompatible.\n",
      "google-documents 0.0.8 requires google-api-python-client==1.6.6, but you have google-api-python-client 1.12.11 which is incompatible.\n",
      "google-documents 0.0.8 requires pandas==0.23.4, but you have pandas 1.3.5 which is incompatible.\n",
      "ml-metadata 1.12.0 requires protobuf<4,>=3.13, but you have protobuf 4.24.4 which is incompatible.\n",
      "ml-pipelines-sdk 1.12.0 requires packaging<21,>=20, but you have packaging 23.2 which is incompatible.\n",
      "ml-pipelines-sdk 1.12.0 requires protobuf<4,>=3.13, but you have protobuf 4.24.4 which is incompatible.\n",
      "tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.24.4 which is incompatible.\n",
      "tensorflow 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.24.4 which is incompatible.\n",
      "tensorflow-data-validation 1.12.0 requires protobuf<4,>=3.13, but you have protobuf 4.24.4 which is incompatible.\n",
      "tensorflow-metadata 1.12.0 requires protobuf<4,>=3.13, but you have protobuf 4.24.4 which is incompatible.\n",
      "tensorflow-model-analysis 0.43.0 requires protobuf<4,>=3.13, but you have protobuf 4.24.4 which is incompatible.\n",
      "tensorflow-serving-api 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.24.4 which is incompatible.\n",
      "tensorflow-transform 1.12.0 requires protobuf<4,>=3.13, but you have protobuf 4.24.4 which is incompatible.\n",
      "tfx-bsl 1.12.0 requires protobuf<4,>=3.13, but you have protobuf 4.24.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed protobuf-4.24.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: google-cloud-documentai in /opt/conda/lib/python3.7/site-packages (2.24.1)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-documentai) (2.17.1)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-documentai) (2.28.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-documentai) (1.22.3)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /opt/conda/lib/python3.7/site-packages (from google-cloud-documentai) (4.24.4)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-documentai) (1.60.0)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-documentai) (2.31.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-documentai) (1.58.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-documentai) (1.48.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-documentai) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-documentai) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-documentai) (4.9)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-documentai) (0.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-documentai) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-documentai) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-documentai) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-documentai) (2024.2.2)\n",
      "\u001b[33mWARNING: Error parsing requirements for fastjsonschema: [Errno 2] No such file or directory: '/opt/conda/lib/python3.7/site-packages/fastjsonschema-2.18.0.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install google-cloud-storage\n",
    "%pip install google-cloud-documentai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2bd0830-3222-4300-b63b-fd21c7b9aa01",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-04-17 11:03:14--  https://raw.githubusercontent.com/GoogleCloudPlatform/document-ai-samples/main/incubator-tools/best-practices/utilities/utilities.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 29735 (29K) [text/plain]\n",
      "Saving to: ‘utilities.py’\n",
      "\n",
      "utilities.py        100%[===================>]  29.04K  --.-KB/s    in 0.002s  \n",
      "\n",
      "2024-04-17 11:03:14 (13.2 MB/s) - ‘utilities.py’ saved [29735/29735]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/GoogleCloudPlatform/document-ai-samples/main/incubator-tools/best-practices/utilities/utilities.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8371a9df-8297-4424-9be0-bfa7cfb9485f",
   "metadata": {},
   "source": [
    "### 2. Import the required libraries/Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ccb6b464-fa6e-42ba-8e1a-feade406fdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path \n",
    "from typing import Dict, List, Union, Optional, Tuple\n",
    "from google.cloud import documentai_v1beta3 as documentai\n",
    "from google.cloud import storage\n",
    "from utilities import file_names,documentai_json_proto_downloader,store_document_as_json\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70e438f-c525-48ef-8de3-fdeda1294a7f",
   "metadata": {},
   "source": [
    "### 3. Input Details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a948aa-b3fb-4b5e-bfd3-058f5e9a38cb",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li><b>input_path :</b> GCS Storage name. It should contain DocAI processed output json files. This bucket is used for processing input files and saving output files in the folders.</li>\n",
    "    <li><b>output_path:</b> GCS URI of the folder, where the Output Json files will store.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "74083216-d1d6-44a6-9b29-967593fb53c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"gs://vignesh_inc_test/date_normalization/output/invoice/\" # Path to your Document AI input JSON files.\n",
    "output_path = \"gs://vignesh_inc_test/date_normalization/post_proc_output_1/\" # Path where Vertex AI output merged JSON files will be saved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28716e4e-210e-4f42-9a0f-fc7d97310567",
   "metadata": {},
   "source": [
    "### 4.Execute the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0fe73f8a-19ff-4025-a082-685ddeb54fc1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************\n",
      "File Name sv202313-0.json\n",
      "---------------\n",
      "Type: due_date\n",
      "Mention Text: 15/12/2023\n",
      "Old Normalized Value: date_value {\n",
      "  year: 2023\n",
      "  month: 12\n",
      "  day: 15\n",
      "}\n",
      "text: \"2023-12-15\"\n",
      "\n",
      "---------------\n",
      "Type: invoice_date\n",
      "Mention Text: 01/12/2023\n",
      "Old Normalized Value: date_value {\n",
      "  year: 2023\n",
      "  month: 1\n",
      "  day: 12\n",
      "}\n",
      "text: \"2023-01-12\"\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:00<00:01,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************\n",
      "File Name google_invoice_edited_date-0.json\n",
      "---------------\n",
      "Type: due_date\n",
      "Mention Text: 05/01/2024\n",
      "Old Normalized Value: date_value {\n",
      "  year: 2024\n",
      "  month: 5\n",
      "  day: 1\n",
      "}\n",
      "text: \"2024-05-01\"\n",
      "\n",
      "---------------\n",
      "Type: invoice_date\n",
      "Mention Text: 01/01/2024\n",
      "Old Normalized Value: date_value {\n",
      "  year: 2024\n",
      "  month: 1\n",
      "  day: 1\n",
      "}\n",
      "text: \"2024-01-01\"\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [00:01<00:00,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************\n",
      "File Name Australia-Invoice-Edited-0.json\n",
      "---------------\n",
      "Type: due_date\n",
      "Mention Text: 05/25/2024\n",
      "Old Normalized Value: date_value {\n",
      "  year: 2024\n",
      "  month: 5\n",
      "  day: 25\n",
      "}\n",
      "text: \"2024-05-25\"\n",
      "\n",
      "---------------\n",
      "Type: invoice_date\n",
      "Mention Text: 03/05/2024\n",
      "Old Normalized Value: date_value {\n",
      "  year: 2024\n",
      "  month: 5\n",
      "  day: 3\n",
      "}\n",
      "text: \"2024-05-03\"\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "All files processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "input_storage_bucket_name = input_path.split(\"/\")[2]\n",
    "input_bucket_path_prefix = \"/\".join(input_path.split(\"/\")[3:])\n",
    "output_storage_bucket_name = output_path.split(\"/\")[2]\n",
    "output_bucket_path_prefix = \"/\".join(output_path.split(\"/\")[3:])\n",
    "\n",
    "def identify_and_convert_date_format(mention_text: str, known_format: Optional[str] = None) -> Tuple[Optional[datetime], str] :\n",
    "    \n",
    "    \"\"\"\n",
    "    This function attempts to identify and convert a date string to a datetime object.\n",
    "\n",
    "    Args:\n",
    "      mention_text: The text string potentially containing a date.\n",
    "      known_format: (Optional) A specific date format string to try first (e.g., \"%Y-%m-%d\").\n",
    "\n",
    "    Returns:\n",
    "      A tuple containing two elements:\n",
    "          - The converted datetime object (or None if not successful).\n",
    "          - The identified date format string (or \"N/A\" if not found).\n",
    "    \"\"\"\n",
    "        \n",
    "    formats = [\"%d/%m/%Y\", \"%m/%d/%Y\"]\n",
    "    if known_format:\n",
    "        formats.insert(0, known_format)\n",
    "\n",
    "    for fmt in formats:\n",
    "        try:\n",
    "            date_obj = datetime.strptime(mention_text, fmt)\n",
    "            return date_obj, fmt\n",
    "        except ValueError:\n",
    "            continue\n",
    "    return None, \"N/A\"\n",
    "\n",
    "def process_json_files(list_of_files : List[str], input_storage_bucket_name : str, output_storage_bucket_name : str, output_bucket_path_prefix : str) -> None:\n",
    "    \"\"\"\n",
    "    Processes a list of JSON files, converting dates within entities to ISO 8601 format and storing the updated JSON data in a specified output bucket.\n",
    "\n",
    "    Args:\n",
    "        list_of_files: List of file paths for the JSON files to process (type: List[str]).\n",
    "        input_storage_bucket_name: Name of the input storage bucket (type: str).\n",
    "        output_storage_bucket_name: Name of the output storage bucket (type: str).\n",
    "        output_bucket_path_prefix: Prefix for the output file paths (type: str).\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    all_json_data = []\n",
    "\n",
    "    for k in tqdm(range(0, len(list_of_files))):\n",
    "        print(\"***************\")\n",
    "        file_name = list_of_files[k].split('/')[-1]  \n",
    "        print(f\"File Name {file_name}\")\n",
    "        json_proto_data = documentai_json_proto_downloader(input_storage_bucket_name, list_of_files[k])\n",
    "        for ind,ent in enumerate(json_proto_data.entities):\n",
    "            if \"date\" in ent.type:\n",
    "                print('---------------')\n",
    "                mention_text = ent.mention_text if hasattr(ent, 'mention_text') else \"\"\n",
    "                normalized_value = ent.normalized_value if hasattr(ent, 'normalized_value') else \"\"\n",
    "                type_ = ent.type if hasattr(ent, 'type') else \"\"\n",
    "                print(f\"Type: {type_}\")\n",
    "                print(f\"Mention Text: {mention_text}\")\n",
    "                print(f\"Old Normalized Value: {normalized_value}\")\n",
    "            \n",
    "                date_obj, identified_format = identify_and_convert_date_format(mention_text)\n",
    "                \n",
    "                json_data = json.loads(documentai.Document.to_json(json_proto_data))\n",
    "\n",
    "                ent_json = json_data['entities'][ind]\n",
    "                    \n",
    "                if date_obj:\n",
    "                    new_date_text_iso = date_obj.strftime(\"%Y-%m-%d\")\n",
    "                    ent_json['normalizedValue']['text'] = new_date_text_iso\n",
    "                    ent_json['normalizedValue']['dateValue'] = {\n",
    "                        'day': date_obj.day,\n",
    "                        'month': date_obj.month,\n",
    "                        'year': date_obj.year\n",
    "                    }\n",
    "                    if identified_format != \"N/A\":\n",
    "                        ent_json['identified_format'] = identified_format\n",
    "                else:\n",
    "                    if identified_format == \"N/A\" and any(e for e in json_data['entities'] if e['type'] == 'date' and 'identified_format' in e):\n",
    "                        known_format = next((e['identified_format'] for e in json_data['entities'] if e['type'] == 'date' and 'identified_format' in e), None)\n",
    "                        date_obj, identified_format = identify_and_convert_date_format(mention_text, known_format=known_format)\n",
    "                        if date_obj:\n",
    "                            new_date_text_iso = date_obj.strftime(\"%Y-%m-%d\")\n",
    "                            ent_json['normalizedValue']['text'] = new_date_text_iso\n",
    "                            ent_json['normalizedValue']['dateValue'] = {\n",
    "                                'day': date_obj.day,\n",
    "                                'month': date_obj.month,\n",
    "                                'year': date_obj.year\n",
    "                            }\n",
    "        \n",
    "   \n",
    "        output_file_name = f\"{output_bucket_path_prefix}{file_name}\"\n",
    "        store_document_as_json(json.dumps(json_data), output_storage_bucket_name, output_file_name)\n",
    "    \n",
    "    print(\"--------------------\")\n",
    "    print(\"All files processed.\")\n",
    "\n",
    "json_files = file_names(input_path)[1].values()\n",
    "list_of_files = [i for i in list(json_files) if i.endswith(\".json\")]\n",
    "process_json_files(list_of_files, input_storage_bucket_name, output_storage_bucket_name, output_bucket_path_prefix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8baad3-4bd5-4626-81b8-b3b5587a5056",
   "metadata": {},
   "source": [
    "### 5.Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cda194-9d54-4d42-9eb0-de225422e17f",
   "metadata": {},
   "source": [
    "The post processed json field can be found in the storage path provided by the user during the script execution that is output_bucket_path. <br><hr>\n",
    "<b>Comparison Between Input and Output File</b><br><br>\n",
    "<i><h4>Post processing results<h4><i><br>\n",
    "Upon running the post processing script against input data. The resultant output json data is obtained. The following image will show the difference date formate in the date filed <br>\n",
    "    \n",
    "\n",
    "<img src= \"./images/output_image1.png\" width=800 height=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace1b20b-6417-420e-b0dd-3841ecac2773",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
